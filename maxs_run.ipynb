{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from interval import interval, inf\n",
    "\n",
    "from brb.attr_input import AttributeInput\n",
    "from brb.brb import csv2BRB"
   ]
  },
  {
   "source": [
    "# Read the Rule Base and create the Expert System"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =  'csv_HPO_BeliefRuleBase_wKO_v15.csv_RefVals_AntImp-1Mglobscaled.csv'\n",
    "filepath = os.path.join('csv_rulebases', filename)\n",
    "\n",
    "assert os.path.exists(filepath), \"rulebase doesn't exist\"\n",
    "\n",
    "# create model from rules.csv\n",
    "model = csv2BRB(filepath,\n",
    "                #'csv_rulebases/csv_ML_BeliefRuleBase_v5.csv_spec_refvals*ant_imp--scaled.csv',\n",
    "                #'csv_rulebases/csv_HPO_BeliefRuleBase_v11.csv_spec_refvals*ant_imp--scaled.csv',\n",
    "                antecedents_prefix='A_',\n",
    "                consequents_prefix='D_',\n",
    "                deltas_prefix='del_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.U_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.U[10].referential_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Trial-ID', 'HPO-library', 'HPO-method', 'ML-algorithm', 'Runs',\n",
       "       'Evaluations', 'Workers', 'GPU', 'Warmstart', 'Wall clock time [s]',\n",
       "       't outperform default [s]', 'Mean (final validation loss)',\n",
       "       'Validation baseline', 'Area under curve (AUC)',\n",
       "       'Mean (final test loss)', 'Test loss ratio (default / best)',\n",
       "       'Test baseline', 'Interquartile range (final test loss)',\n",
       "       't best configuration [s]', 'Generalization error',\n",
       "       'Evaluations for best configuration', 'Crashes', '# training instances',\n",
       "       '# training features', '# test instances', '# test features', 'dataset',\n",
       "       '# cont. HPs', '# int. HPs', '# cat. HPs', 'loss_metric'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "dataset = 'turbofan'\n",
    "file_name = 'expanded_metrics_' + dataset + '.csv'\n",
    "maxr_fpath = os.path.join('max_results', file_name)\n",
    "\n",
    "maxr = pd.read_csv(maxr_fpath, index_col=0)\n",
    "\n",
    "# no GPU experiments // antecedent not available in Philipp's rule base\n",
    "maxr = maxr[maxr['GPU'] == False]\n",
    "\n",
    "maxr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input generation for the Expert System (Use Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between BRB and BM notation\n",
    "\n",
    "# Map ML algorithms: BRB -> BM\n",
    "# TODO: Needs to be expanded to include classification algorithms\n",
    "ml_brb2bm_map = {\n",
    "    'Ada Boost': 'AdaBoostRegressor',\n",
    "    'Decision Tree': 'DecisionTreeRegressor',\n",
    "    'Support Vector Machine': 'SVR',\n",
    "    'KNN': 'KNNRegressor',\n",
    "    'Gradient Boosting Machine': 'LGBMRegressor',\n",
    "    'Random Forest': 'RandomForestRegressor',\n",
    "    'XGBoost': 'XGBoostRegressor',\n",
    "    'ElasticNet': 'ElasticNet',\n",
    "}\n",
    "\n",
    "# Map ML algorithms: BM -> BRB\n",
    "ml_bm2brb_map = {v: k for k, v in ml_brb2bm_map.items()}\n",
    "\n",
    "# Map HPO techniques: BRB -> BM\n",
    "hpo_brb2bm_map = {\n",
    "    'BOHAMIANN': 'Bohamiann',\n",
    "    'BOHB': 'BOHB',\n",
    "    'CMA-ES': 'CMA-ES',\n",
    "    'FABOLAS': 'Fabolas',\n",
    "    'GPBO': 'GPBO',\n",
    "    'HB': 'Hyperband',\n",
    "    'Random Search': 'RandomSearch',\n",
    "    'SMAC': 'SMAC',\n",
    "    'TPE': 'TPE',\n",
    "    'Default Values': 'Default Values'\n",
    "}\n",
    "\n",
    "# Map HPO techniques: BM -> BRB\n",
    "hpo_bm2brb_map = {v: k for k, v in hpo_brb2bm_map.items()}\n",
    "\n",
    "# Map warm tart notation: BRB -> BM\n",
    "wst_brb2bm_map = {'yes': True, 'no': False}\n",
    "\n",
    "wst_bm2brb_map = {v: k for k, v in wst_brb2bm_map.items()}\n",
    "\n",
    "# Map ML algorithms with HP data types\n",
    "bmalgo2paratype_map = {\n",
    "    'RandomForestRegressor': '[continuous, discrete, nominal]',\n",
    "    'RandomForestClassifier': '[continuous, discrete, nominal]',\n",
    "    'MLPRegressor': '[discrete, nominal]',\n",
    "    'MLPClassifier': '[discrete, nominal]',\n",
    "    'SVR': '[continuous, nominal]',\n",
    "    'SVC': '[continuous, nominal]',\n",
    "    'KerasRegressor': '[continuous, discrete, nominal]',\n",
    "    'KerasClassifier': '[continuous, discrete, nominal]',\n",
    "    'XGBoostClassifier': '[continuous, discrete, nominal]',\n",
    "    'XGBoostRegressor': '[continuous, discrete, nominal]',\n",
    "    'AdaBoostRegressor': '[continuous, discrete, nominal]',\n",
    "    'AdaBoostClassifier': '[continuous, discrete, nominal]',\n",
    "    'DecisionTreeRegressor': '[continuous, discrete]',\n",
    "    'DecisionTreeClassifier': '[continuous, discrete]',\n",
    "    'LinearRegression': '[nominal]',\n",
    "    'KNNRegressor': '[discrete, nominal]',\n",
    "    'KNNClassifier': '[discrete, nominal]',\n",
    "    'LGBMRegressor': '[continuous, discrete]',\n",
    "    'LGBMClassifier': '[continuous, discrete]',\n",
    "    'LogisticRegression': '[continuous, discrete, nominal]',\n",
    "    'ElasticNet': '[continuous, discrete, nominal]'}\n",
    "\n",
    "# Map ML algorithms with conditional / non-conditional HPs\n",
    "bmalgo2cond_map = {\n",
    "    'RandomForestRegressor': 'no',\n",
    "    'RandomForestClassifier': 'no',\n",
    "    'MLPRegressor': 'no',\n",
    "    'MLPClassifier': 'no',\n",
    "    'SVR': 'no',\n",
    "    'SVC': 'no',\n",
    "    'KerasRegressor': 'no',\n",
    "    'KerasClassifier': 'no',\n",
    "    'XGBoostClassifier': 'yes',\n",
    "    'XGBoostRegressor': 'yes',\n",
    "    'AdaBoostRegressor': 'no',\n",
    "    'AdaBoostClassifier': 'no',\n",
    "    'DecisionTreeRegressor': 'no',\n",
    "    'DecisionTreeClassifier': 'no',\n",
    "    'LinearRegression': 'no',\n",
    "    'KNNRegressor': 'no',\n",
    "    'KNNClassifier': 'no',\n",
    "    'LGBMRegressor': 'no',\n",
    "    'LGBMClassifier': 'no',\n",
    "    'LogisticRegression': 'no',\n",
    "    'ElasticNet': 'no'}\n",
    "\n",
    "# Map dataset information with some (constant) antecedents\n",
    "datset2constant_map = {\n",
    "    'turbofan': {\n",
    "        \"Detailed ML task\": 'Prediction of Remaining Useful Lifetime',\n",
    "        \"Production application area\": 'Predictive Maintenance',\n",
    "        \"Input Data\": 'Tabular Data',\n",
    "        \"Ratio training to test dataset\": 4,\n",
    "        \"ML task\": 'Regression'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the use cases for the evaluation of the BRBES (each row corresponds to a single use case)\n",
    "df_use_case = pd.DataFrame([])\n",
    "df_use_case['Machine Learning Algorithm'] = maxr['ML-algorithm'].map(ml_bm2brb_map)\n",
    "df_use_case['Hardware: Number of workers/kernels for parallel computing'] = maxr['Workers']\n",
    "df_use_case['Availability of a warm-start HP configuration'] = maxr['Warmstart'].map(wst_bm2brb_map)\n",
    "df_use_case['Number of maximum function evaluations/ trials budget'] = maxr['Evaluations']\n",
    "df_use_case['Running time per trial [s]'] = [interval[0, 30]] * len(maxr['ML-algorithm']) # TODO: Calculation necessary\n",
    "df_use_case['Total Computing Time [s]'] = maxr['Wall clock time [s]']\n",
    "df_use_case['Dimensionality of HPs'] = maxr['# cont. HPs'] + maxr['# int. HPs'] + maxr['# cat. HPs']\n",
    "df_use_case['HP datatypes'] = maxr['ML-algorithm'].map(bmalgo2paratype_map)\n",
    "df_use_case['Conditional HP space'] = maxr['ML-algorithm'].map(bmalgo2cond_map)\n",
    "df_use_case[\"Detailed ML task\"] = datset2constant_map[dataset][\"Detailed ML task\"]\n",
    "df_use_case[\"Production application area\"] = datset2constant_map[dataset][\"Production application area\"]\n",
    "df_use_case['Input Data'] = datset2constant_map[dataset][\"Input Data\"]\n",
    "df_use_case['#Instances training dataset'] = maxr['# training instances']\n",
    "df_use_case['Ratio training to test dataset'] = datset2constant_map[dataset][\"Ratio training to test dataset\"]\n",
    "df_use_case['ML task'] = datset2constant_map[dataset][\"ML task\"]\n",
    "\n",
    "# fixed antecedents (cannot yet be derived from the metrics .csv file)\n",
    "df_use_case[\"UR: quality demands\"] = 'high'\n",
    "df_use_case[\"User's programming ability\"] = ''\n",
    "df_use_case[\"UR: Computer operating system\"] = 'Linux'\n",
    "df_use_case[\"Obtainability of good approximate\"] = [{'yes':0.5, 'no':0.5}] * len(maxr['ML-algorithm'])\n",
    "df_use_case[\"Supports parallel evaluations\"] = [{'yes':0.5, 'no':0.5}] * len(maxr['ML-algorithm'])\n",
    "df_use_case[\"Obtainability of gradients\"] = [{'yes':0.5, 'no':0.5}] * len(maxr['ML-algorithm'])\n",
    "df_use_case[\"Noise in dataset\"] = [{'yes':0.5, 'no':0.5}] * len(maxr['ML-algorithm'])\n",
    "df_use_case[\"Training Technique\"] = \"Offline\"\n",
    "df_use_case[\"UR: need for model transparency\"] = ''\n",
    "df_use_case[\"UR: Availability of a well documented library\"] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate use cases\n",
    "col_subset = df_use_case.columns\n",
    "col_subset = col_subset[:-7]\n",
    "df_use_case.drop_duplicates(subset=col_subset, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use_case.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the BRBES for the BM use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the brb model for the use cases from the BM study on compute the beliefs and rankings of the HPO techniques\n",
    "\n",
    "df_belief = pd.DataFrame([])\n",
    "df_rank = pd.DataFrame([])\n",
    "\n",
    "for idx, row in df_use_case.iterrows():\n",
    "    \n",
    "    use_case_dict = row.to_dict()\n",
    "    use_case_dict = {'A_' + k: v for k, v in use_case_dict.items()}\n",
    "\n",
    "    X = AttributeInput(use_case_dict)\n",
    "\n",
    "    # run brb model\n",
    "    belief_degrees = model.run(X)\n",
    "    belief_degrees = {k[2:]: v for k, v in zip(model.D, belief_degrees)}\n",
    "\n",
    "    # append results\n",
    "    df_belief = df_belief.append(belief_degrees, ignore_index=True)\n",
    "    \n",
    "    hpo_beliefs = {v: k for k, v in belief_degrees.items()}\n",
    "    hpo_beliefs = sorted(hpo_beliefs.items(), reverse=True)\n",
    "\n",
    "    # BEWARE: THE RANK IS 0-BASED\n",
    "    hpo_ranks = {hpo_belief[-1]: i for i, hpo_belief in enumerate(hpo_beliefs)}\n",
    "    \n",
    "    df_rank = df_rank.append(hpo_ranks, ignore_index=True)\n",
    "\n",
    "df_use_case.to_csv('use_cases.csv')\n",
    "df_belief.to_csv('hpo_beliefs.csv')\n",
    "df_rank.to_csv('hpo_ranks.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_belief.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_belief.idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation to max results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BRBES avg. score (stdev): \t\t0.091 (0.16)\nRandomSearch avg. score (stdev): \t0.167 (0.24)\nDefault Values avg. score (stdev): \t0.806 (0.36)\n"
     ]
    }
   ],
   "source": [
    "brb_scores = list()  # Stores the scaled score achieved by the BRBES in each use case\n",
    "rs_scores = list()  # Stores the scaled score achieved by RS in each use case\n",
    "dv_scores = list()  # Stores the scaled score achieved by the Default HPs in each use case\n",
    "\n",
    "summary_df = df_use_case.copy(deep=True)\n",
    "\n",
    "for idx, use_case in df_use_case.iterrows():\n",
    "\n",
    "    # Identify the experiments (from the benchmarking study), that correspond to this specific use case\n",
    "    exp = maxr.loc[(maxr['ML-algorithm'] == ml_brb2bm_map[use_case['Machine Learning Algorithm']]) &\n",
    "        (maxr['Workers'] == use_case['Hardware: Number of workers/kernels for parallel computing']) &\n",
    "        (maxr['Warmstart'] == wst_brb2bm_map[use_case['Availability of a warm-start HP configuration']]) &\n",
    "        (maxr['Wall clock time [s]'] == use_case['Total Computing Time [s]']), :] # TODO: Use additional antecedents here to distinguish between different data sets (especially GPU/CPU)\n",
    "\n",
    "    if len(exp) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute the scaled loss deviation for each HPO technique in this experiment\n",
    "    hpos = exp.set_index('HPO-method')['Mean (final validation loss)'] # TODO: Differentiate between Test and Validation loss here?\n",
    "    hpos.loc['Default Values'] = np.nanmean(exp['Validation baseline'])\n",
    "    loss_arr = hpos.to_numpy()\n",
    "    min_value = np.nanmin(loss_arr)\n",
    "    max_value = np.nanmax(loss_arr[loss_arr != np.inf])\n",
    "    scaled_hpos = (hpos - min_value) / (max_value - min_value)\n",
    "\n",
    "    rec_hpo = df_belief.iloc[idx].idxmax(axis='columns')\n",
    "\n",
    "    brb_scores.append(scaled_hpos.loc[hpo_brb2bm_map[rec_hpo]])\n",
    "    rs_scores.append(scaled_hpos.loc['RandomSearch'])\n",
    "    dv_scores.append(scaled_hpos.loc['Default Values'])\n",
    "\n",
    "    summary_df.loc[idx, 'BRBES Recommendation'] = hpo_brb2bm_map[rec_hpo]\n",
    "    summary_df.loc[idx, 'Best HPO Technique'] = scaled_hpos.idxmin(axis=0)\n",
    "    summary_df.loc[idx, 'Distance Value'] = scaled_hpos.loc[hpo_brb2bm_map[rec_hpo]]\n",
    "\n",
    "print('BRBES avg. score (stdev): \\t\\t{:.3f} ({:.2f})'.format(np.mean(brb_scores), np.std(brb_scores)))\n",
    "print('RandomSearch avg. score (stdev): \\t{:.3f} ({:.2f})'.format(np.mean(rs_scores), np.std(rs_scores)))\n",
    "print('Default Values avg. score (stdev): \\t{:.3f} ({:.2f})'.format(np.mean(dv_scores), np.std(dv_scores)))\n",
    "\n",
    "summary_fname = dataset + '_brbes_results.csv'\n",
    "summary_df.to_csv(summary_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}